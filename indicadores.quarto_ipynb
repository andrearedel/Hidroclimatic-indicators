{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Automated generation of hydroclimatic indicators\n",
        "author: \"Andrea Redel\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    theme: sandstone\n",
        "    code-tools: true\n",
        "  pdf: default\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "The following workflow was developed as part of an academic residency for the undergraduate Forestry Engineering program under the supervision of Dr. Isabel Rojas.\n",
        "\n",
        "This repository contains Python scripts designed to filter, organize, and prepare daily hydrometeorological data from the *CAMELS-CL* dataset for further analysis, such as with *Climpact* and *Indicators of Hydrologic Alteration (IHA)* tools. Climpact generates **33 climate indicators** to assess climate change at each meteorological station. IHA (Indicators of Hydrologic Alteration) provides **33 indicators of hydrologic alteration** and **34 indicators related to components of ecological flow**, for each streamflow (hydrometric) station. This script allows for the **systematic preparation of results** from each station within a watershed into selected graphical figures.\n",
        "\n",
        "Some variable names and code comments are in Spanish as the original dataset and workflow were developed using Chilean climatic and hydrological data. However, all documentation, figures, and explanations are provided in English for international use. The code can be easily adapted to other languages or datasets. Variable names in Spanish follow the structure of the national dataset (Chile), but can be replaced as needed.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The scripts perform the following main tasks:\n",
        "\n",
        "1.  **Filter CAMELS-CL data** (`tmax`, `tmin`, `precip`, and `q`) for specific stations based on station codes\n",
        "\n",
        "2.  **Create CSV files** with `year`, `month`, and `day` columns plus values for each station.\n",
        "\n",
        "3.  **Split data by station**, saving individual files per station for each variable.\n",
        "\n",
        "4.  **Format data** for compatibility with Climpact and IHA:\n",
        "\n",
        "-   Remove `NA` and `inf`/`-inf` values.\n",
        "\n",
        "-   Remove column headers.\n",
        "\n",
        "-   Format output into required directory structure.\n",
        "\n",
        "5.  Generate **custom climate indicator figures** from the output files generated by the Climpact tool.\n",
        "\n",
        "6.  Generate **custom hydrologic indicator figures** from the output files generated by the IHA tool.\n",
        "\n",
        "## How to use\n",
        "\n",
        "1.  Clone this repository and open it in a Python-capable editor (recommended: Visual Studio Code).\n",
        "\n",
        "2.  Download here the working directory.\n",
        "\n",
        "3.  Run the Preprocessing Data script.\n",
        "\n",
        "4.  Use the generated products to import them into Climpact and IHA software.\n",
        "\n",
        "5.  Run the script for automated generation of figures for climate and fluvial indicators\n",
        "\n",
        "## Indicators\n",
        "\n",
        "In the presentation of figures in this work, a selection of climatic and hydrological indicators was made based on their ability to assess climate change and their ecological relevance.\n",
        "\n",
        "1.  Climatic indicators\n",
        "\n",
        "-   Precipitation intensity (@fig-polar-2)\n",
        "\n",
        "-   Annual precipitation (@fig-polar-3)\n",
        "\n",
        "-   Consecutive dry days (@fig-polar-4)\n",
        "\n",
        "-   Days with precipitation \\>= 30 mm (@fig-polar-5)\n",
        "\n",
        "-   Standardised Precipitation Evapostranspiration Index (@fig-polar-6)\n",
        "\n",
        "-   Annual mean daily minimum temperature (@fig-polar-7)\n",
        "\n",
        "-   Annual mean daily maximum temperature (@fig-polar-8)\n",
        "\n",
        "-   Annual warmest daily maximum temperature (@fig-polar-9)\n",
        "\n",
        "-   Days when minimum temperature \\< 0ºC (@fig-polar-10)\n",
        "\n",
        "2.  Statistical analysis of flow rates\n",
        "\n",
        "-   Flow hydrograph (@fig-polar-11)\n",
        "\n",
        "-   Boxplot of median monthly flow (@fig-polar-12)\n",
        "\n",
        "-   Percentiles of monthly flow (@fig-polar-13)\n",
        "\n",
        "-   Flow Duration Curve (@fig-polar-14)\n",
        "\n",
        "-   Median monthly flow of the entire basin (@fig-polar-15)\n",
        "\n",
        "3.  Time series of hydrological indicators\n",
        "\n",
        "-   Median flow in July (@fig-polar-16)\n",
        "\n",
        "-   High pulse duration (@fig-polar-17)\n",
        "\n",
        "-   Base flow index (@fig-polar-18)\n",
        "\n",
        "-   Small flood peak (@fig-polar-19)\n",
        "\n",
        "-   Large flood peak (@fig-polar-20)\n",
        "\n",
        "## Hydroclimatic Data Preprocessing Script\n",
        "\n",
        "### Filter CAMELS-CL data\n",
        "\n",
        "4.1.1. Dependencies and workspace\n",
        "\n",
        "4.1.1.1 Import libraries"
      ],
      "id": "35c16ddf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "id": "cbb4a0c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.1.2. Make sure the directory exists"
      ],
      "id": "627524ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def asegurar_directorio(ruta):\n",
        "    os.makedirs(ruta, exist_ok=True)"
      ],
      "id": "044f17b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.1.3. Define folder paths"
      ],
      "id": "cf59b4ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.chdir('/Users/andrearedel/Documents/Retoño')"
      ],
      "id": "338d31d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ruta_base = 'Hidroclima/Base de datos'\n",
        "ruta_guardado = os.path.join(ruta_base, '0.filtrado_estaciones')"
      ],
      "id": "b260e077",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.1.4. Create the directory if it doesn't exist"
      ],
      "id": "e0e064cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "asegurar_directorio(ruta_guardado)"
      ],
      "id": "d3220013",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.1.5. Additional columns"
      ],
      "id": "338da478"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columnas_adicionales = ['year', 'month', 'day']"
      ],
      "id": "d4df9667",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.2. Maximum temperature filter 4.1.2.1. Read the CSV file"
      ],
      "id": "24497f18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(os.path.join(ruta_base, 'CAMELS_CL_v202201/tmax_cr2met_C_day.csv'))"
      ],
      "id": "915a7c68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.2.2. Filter columns based on River Basin"
      ],
      "id": "78d3b132"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columnas_filtradas = [col for col in df.columns if col.isdigit() and 9100000 <= int(col) <= 9199999]\n",
        "columnas_filtradas += [col for col in columnas_adicionales if col in df.columns]"
      ],
      "id": "1f22477a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this article, we will use the Toltén River Basin, Chile, identified by the code `94`, as a case study.\n",
        "\n",
        "To analyze a different basin:\n",
        "\n",
        "-   Change the station code range, for example, the Imperial River Basin, identified by the code `91`.\n",
        "-   Update all relevant parts of the script accordingly."
      ],
      "id": "92035a9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columnas_filtradas = [col for col in df.columns if col.isdigit() and 9400000 <= int(col) <= 9499999]\n",
        "columnas_filtradas += [col for col in columnas_adicionales if col in df.columns]"
      ],
      "id": "e45f9c69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.2.3. Create a new Data Frame with the filtered columns"
      ],
      "id": "a3925c94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_filtrado = df[columnas_filtradas]"
      ],
      "id": "330ccdfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.2.4. Save the new Data Frame to a CSV file"
      ],
      "id": "b7ddd077"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-1\n",
        "#| tbl-cap: Maximum temperature filter for the stations in each basin\n",
        "df_filtrado.to_csv(os.path.join(ruta_guardado, 'tmax.csv'), index=False)\n",
        "\n",
        "print(\"Filtered columns and new file of maximum temperatures for the basin's stations saved.\")\n",
        "\n",
        "print(\"Preview:\")\n",
        "df.iloc[:5, :10]"
      ],
      "id": "tbl-polar-1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.3. Minimum Temperature filter The same process is repeated as with the maximum temperature."
      ],
      "id": "bf677ddb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-2\n",
        "#| tbl-cap: Minimum temperature filter for the stations in each basin\n",
        "df = pd.read_csv(os.path.join(ruta_base, 'CAMELS_CL_v202201/tmin_cr2met_C_day.csv'))\n",
        "\n",
        "columnas_filtradas = [col for col in df.columns if col.isdigit() and 9400000 <= int(col) <= 9499999]\n",
        "columnas_filtradas += [col for col in columnas_adicionales if col in df.columns]\n",
        "\n",
        "df_filtrado = df[columnas_filtradas]\n",
        "\n",
        "df_filtrado.to_csv(os.path.join(ruta_guardado, 'tmin.csv'), index=False)\n",
        "\n",
        "print(\"Filtered columns and new file of minimum temperatures for the basin's stations saved.\")\n",
        "\n",
        "print(\"Preview:\")\n",
        "df.iloc[:5, :10]"
      ],
      "id": "tbl-polar-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.4. Precipitation filter The same process is repeated."
      ],
      "id": "ab9efe6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-3\n",
        "#| tbl-cap: Precipitation filter for the stations in each basin\n",
        "df = pd.read_csv(os.path.join(ruta_base, 'CAMELS_CL_v202201/precip_cr2met_mm_day.csv'))\n",
        "\n",
        "columnas_filtradas = [col for col in df.columns if col.isdigit() and 9400000 <= int(col) <= 9499999]\n",
        "columnas_filtradas += [col for col in columnas_adicionales if col in df.columns]\n",
        "\n",
        "df_filtrado = df[columnas_filtradas]\n",
        "\n",
        "df_filtrado.to_csv(os.path.join(ruta_guardado, 'pp.csv'), index=False)\n",
        "\n",
        "print(\"Filtered columns and new file of precipitation for the basin's stations saved.\")\n",
        "\n",
        "print(\"Preview:\")\n",
        "df.iloc[:5, :10]"
      ],
      "id": "tbl-polar-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1.5. Streamflow filter The same process is repeated."
      ],
      "id": "c418deb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-4\n",
        "#| tbl-cap: Streamflow filter for the stations in each basin\n",
        "df = pd.read_csv(os.path.join(ruta_base, 'CAMELS_CL_v202201/q_m3s_day.csv'))\n",
        "\n",
        "columnas_filtradas = [col for col in df.columns if col.isdigit() and 9400000 <= int(col) <= 9499999]\n",
        "columnas_filtradas += [col for col in columnas_adicionales if col in df.columns]\n",
        "\n",
        "df_filtrado = df[columnas_filtradas]\n",
        "\n",
        "df_filtrado.to_csv(os.path.join(ruta_guardado, 'q.csv'), index=False)\n",
        "\n",
        "print(\"Filtered columns and new file of streamflow for the basin's stations saved.\")\n",
        "\n",
        "print(\"Preview:\")\n",
        "df.iloc[:5, :10]"
      ],
      "id": "tbl-polar-4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create separate CSV files for each station and variable\n",
        "\n",
        "4.2.1. Minimum Temperature 4.2.1.1. CSV file path"
      ],
      "id": "469b2be8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "archivo_csv = 'Hidroclima/Base de datos/0.filtrado_estaciones/tmin.csv'"
      ],
      "id": "36e87509",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.1.2. Read CSV file"
      ],
      "id": "3729be95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(archivo_csv)"
      ],
      "id": "cd5d9641",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.1.3. Get the Data Frame columns"
      ],
      "id": "820bb947"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columnas = df.columns"
      ],
      "id": "4de31944",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.1.4. Create a directory to save files by column"
      ],
      "id": "88e44aea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_dir = 'Hidroclima/Base de datos/1.temperaturas_min'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "id": "c85bae0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.1.5. Iterate over the columns and save the data for `year`, `month,` and `day`"
      ],
      "id": "6ffca605"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-5\n",
        "#| tbl-cap: Daily minimum temperature for station 9437002\n",
        "for col in columnas:\n",
        "    df_col = df[['year', 'month', 'day', col]].copy()\n",
        "    df_col = df_col.rename(columns={col: 'tmin'})\n",
        "    \n",
        "    output_path = os.path.join(output_dir, f'{col}.csv')\n",
        "    df_col.to_csv(output_path, index=False)\n",
        "    \n",
        "print(\"Files of minimum temperature (tmin) separated by station were successfully generated.\")\n",
        "\n",
        "archivos_generados = os.listdir(output_dir)\n",
        "print(\"Generated_files:\")\n",
        "print(archivos_generados)\n",
        "\n",
        "primer_archivo = archivos_generados[1]\n",
        "ruta_primero = os.path.join(output_dir, primer_archivo)\n",
        "\n",
        "df_primero = pd.read_csv(ruta_primero)\n",
        "print(f\"First file: station {primer_archivo}\")\n",
        "display(df_primero.head())"
      ],
      "id": "tbl-polar-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.2. Maximum Temperature The same process is repeated"
      ],
      "id": "1545514f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-6\n",
        "#| tbl-cap: Daily maximum temperature for station 9437002\n",
        "archivo_csv = 'Hidroclima/Base de datos/0.filtrado_estaciones/tmax.csv'\n",
        "\n",
        "df = pd.read_csv(archivo_csv)\n",
        "\n",
        "columnas = df.columns\n",
        "\n",
        "output_dir = 'Hidroclima/Base de datos/2.temperaturas_max'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for col in columnas:\n",
        "    df_col = df[['year', 'month', 'day', col]].copy()\n",
        "    df_col = df_col.rename(columns={col: 'tmax'})\n",
        "    \n",
        "    output_path = os.path.join(output_dir, f'{col}.csv')\n",
        "    df_col.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Files of maximum temperature (tmax) separated by station were successfully generated.\")\n",
        "\n",
        "archivos_generados = os.listdir(output_dir)\n",
        "print(\"Generated_files:\")\n",
        "print(archivos_generados)\n",
        "\n",
        "primer_archivo = archivos_generados[1]\n",
        "ruta_primero = os.path.join(output_dir, primer_archivo)\n",
        "\n",
        "df_primero = pd.read_csv(ruta_primero)\n",
        "print(f\"First file: station {primer_archivo}\")\n",
        "display(df_primero.head())"
      ],
      "id": "tbl-polar-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.3. Precipitation The same process is repeated"
      ],
      "id": "02c963bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-7\n",
        "#| tbl-cap: Daily precipitation for station 9437002\n",
        "archivo_csv = 'Hidroclima/Base de datos/0.filtrado_estaciones/pp.csv'\n",
        "\n",
        "df = pd.read_csv(archivo_csv)\n",
        "\n",
        "columnas = df.columns\n",
        "\n",
        "output_dir = 'Hidroclima/Base de datos/3.precipitaciones'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for col in columnas:\n",
        "    df_col = df[['year', 'month', 'day', col]].copy()\n",
        "    df_col = df_col.rename(columns={col: 'pp'})\n",
        "    \n",
        "    output_path = os.path.join(output_dir, f'{col}.csv')\n",
        "    df_col.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Files of precipitation (pp) separated by station were successfully generated.\")\n",
        "\n",
        "archivos_generados = os.listdir(output_dir)\n",
        "print(\"Generated_files:\")\n",
        "print(archivos_generados)\n",
        "\n",
        "primer_archivo = archivos_generados[1]\n",
        "ruta_primero = os.path.join(output_dir, primer_archivo)\n",
        "\n",
        "df_primero = pd.read_csv(ruta_primero)\n",
        "print(f\"First file: station {primer_archivo}\")\n",
        "display(df_primero.head())"
      ],
      "id": "tbl-polar-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.4. Streamflow The same process is repeated"
      ],
      "id": "8e780924"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-8\n",
        "#| tbl-cap: Daily streamflow for station 9437002\n",
        "archivo_csv = 'Hidroclima/Base de datos/0.filtrado_estaciones/q.csv'\n",
        "\n",
        "df = pd.read_csv(archivo_csv)\n",
        "\n",
        "columnas = df.columns\n",
        "\n",
        "output_dir = 'Hidroclima/Base de datos/7.caudales'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for col in columnas:\n",
        "    df_col = df[['year', 'month', 'day', col]].copy()\n",
        "    df_col = df_col.rename(columns={col: 'q'})\n",
        "    \n",
        "    output_path = os.path.join(output_dir, f'{col}.csv')\n",
        "    df_col.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Files of streamflow (q) separated by station were successfully generated.\")\n",
        "\n",
        "archivos_generados = os.listdir(output_dir)\n",
        "print(\"Generated_files:\")\n",
        "print(archivos_generados)\n",
        "\n",
        "primer_archivo = archivos_generados[1]\n",
        "ruta_primero = os.path.join(output_dir, primer_archivo)\n",
        "\n",
        "df_primero = pd.read_csv(ruta_primero)\n",
        "print(f\"First file: station {primer_archivo}\")\n",
        "display(df_primero.head())"
      ],
      "id": "tbl-polar-8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split data by station\n",
        "\n",
        "4.3.1. Format data for compability with Climpact 4.3.1.1. Mapping precipitation, minimum and maximum temperature based on filename and file path"
      ],
      "id": "60d9f392"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dir_principal = 'Hidroclima/Base de datos'\n",
        "\n",
        "subcarpetas = ['1.temperaturas_min', '2.temperaturas_max', '3.precipitaciones']\n",
        "\n",
        "columna_map = {\n",
        "    '1.temperaturas_min': 'tmin',\n",
        "    '2.temperaturas_max': 'tmax',\n",
        "    '3.precipitaciones': 'prcp'\n",
        "}\n",
        "\n",
        "def obtener_archivos(subcarpeta):\n",
        "    ruta_subcarpeta = os.path.join(dir_principal, subcarpeta)\n",
        "    archivos = [archivo for archivo in os.listdir(ruta_subcarpeta) if archivo.endswith('.csv') and archivo not in ['year.csv', 'month.csv', 'day.csv']]\n",
        "    return archivos\n",
        "\n",
        "def leer_archivo(ruta, columna):\n",
        "    df = pd.read_csv(ruta)\n",
        "    df = df.rename(columns={df.columns[-1]: columna})\n",
        "    return df\n",
        "\n",
        "estaciones_data = {}\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    columna = columna_map[subcarpeta]\n",
        "    archivos = obtener_archivos(subcarpeta)\n",
        "    \n",
        "    for archivo in archivos:\n",
        "        estacion = os.path.splitext(archivo)[0]\n",
        "        ruta_archivo = os.path.join(dir_principal, subcarpeta, archivo)\n",
        "        \n",
        "        df = leer_archivo(ruta_archivo, columna)\n",
        "        \n",
        "        if estacion not in estaciones_data:\n",
        "            estaciones_data[estacion] = df[['year', 'month', 'day', columna]]\n",
        "        else:\n",
        "            estaciones_data[estacion] = estaciones_data[estacion].merge(df[['year', 'month', 'day', columna]], on=['year', 'month', 'day'], how='outer')"
      ],
      "id": "bde78827",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3.1.2. Save files by station"
      ],
      "id": "7f1ada95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_dir = os.path.join(dir_principal, '4.pre_Climpact')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for estacion, df in estaciones_data.items():\n",
        "    for columna in ['prcp', 'tmax', 'tmin']:\n",
        "        if columna not in df.columns:\n",
        "            df[columna] = pd.NA\n",
        "    \n",
        "    df = df[['year', 'month', 'day', 'prcp', 'tmax', 'tmin']]"
      ],
      "id": "3e1f6caa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3.1.3. Format:\n",
        "\n",
        "-   Approximate variable values to 1 decimal place\n",
        "\n",
        "-   Convert `day`and `month` values to 2 digits\n",
        "\n",
        "-   Combine `year`, `month` and `day` into a single 'date' column\n",
        "\n",
        "-   Replace `inf`, `-inf` and `NaN` values ​​with `-99.9`\n",
        "\n",
        "-   Export as a `.txt` file\n",
        "\n",
        "-   Correct order of columns"
      ],
      "id": "9ac2e346"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-9\n",
        "#| tbl-cap: List of the generated files in the output directory\n",
        "   \n",
        "    df[['prcp', 'tmax', 'tmin']] = df[['prcp', 'tmax', 'tmin']].apply(lambda x: round(x, 1))\n",
        "    \n",
        "    df['month'] = df['month'].apply(lambda x: f'{x:02}')\n",
        "    df['day'] = df['day'].apply(lambda x: f'{x:02}')\n",
        "    \n",
        "    df['fecha'] = df.apply(lambda row: f\"{int(row['year']):4d} {row['month']} {row['day']}\", axis=1)\n",
        "    \n",
        "    df = df.replace([float('inf'), float('-inf')], -99.9).fillna(-99.9)\n",
        "    \n",
        "    ruta_salida = os.path.join(output_dir, f'{estacion}.txt')\n",
        "    \n",
        "    with open(ruta_salida, 'w') as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['fecha']}    {row['prcp']:5.1f}     {row['tmax']:4.1f}     {row['tmin']:4.1f}\\n\")\n",
        "\n",
        "print(\"Files for Climpact generated successfully.\")\n",
        "\n",
        "archivos_generados = sorted(os.listdir(output_dir))\n",
        "df_archivos = pd.DataFrame(archivos_generados, columns=['Generated files'])\n",
        "display(df_archivos)"
      ],
      "id": "tbl-polar-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-10\n",
        "#| tbl-cap: Station 9437002 file with information on each variable\n",
        "#| echo: false\n",
        "\n",
        "from IPython.display import HTML\n",
        "html = df_primero.head().to_html(index=False, header=False)\n",
        "display(HTML(html))"
      ],
      "id": "tbl-polar-10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3.2. Format data for compability with IHA 4.3.2.1. The same process is repeated, now for the streamflow."
      ],
      "id": "9099bb08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dir_entrada = os.path.join('Hidroclima', 'Base de datos', '7.caudales')\n",
        "dir_salida = os.path.join('Hidroclima', 'Base de datos', '8.pre_IHA')\n",
        "\n",
        "os.makedirs(dir_salida, exist_ok=True)\n",
        "\n",
        "for archivo in os.listdir(dir_entrada):\n",
        "    if archivo.endswith('.csv'):\n",
        "        ruta_entrada = os.path.join(dir_entrada, archivo)\n",
        "        \n",
        "        archivo_salida = os.path.splitext(archivo)[0] + '.txt'\n",
        "        ruta_salida = os.path.join(dir_salida, archivo_salida)\n",
        "\n",
        "        df = pd.read_csv(ruta_entrada)\n",
        "\n",
        "        columnas_requeridas = {'year', 'month', 'day', 'q'}\n",
        "        if not columnas_requeridas.issubset(df.columns):\n",
        "            print(f\"The {archivo} file does not contain the required columns: {columnas_requeridas}\")\n",
        "            continue\n",
        "\n",
        "        df = df[['year', 'month', 'day', 'q']]"
      ],
      "id": "3239d29a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3.2.2. Format:\n",
        "\n",
        "-   Aproximate streamflow values to 1 decimal place\n",
        "\n",
        "-   Date column in **YYYY-MM-DD** format\n",
        "\n",
        "-   Replace `inf`, `-inf`, and `NaN` values ​​with `-1.0`\n",
        "\n",
        "-   Delete leading rows where 'q' equals -1.0\n",
        "\n",
        "-   Streamflow column named `flow`\n",
        "\n",
        "-   Export as `.txt`file"
      ],
      "id": "32de4615"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "        df = df.replace([float('inf'), float('-inf')], -1.0).fillna(-1.0)\n",
        "\n",
        "        df = df.loc[df['q'] != -1.0].reset_index(drop=True)\n",
        "\n",
        "        df['q'] = df['q'].round(1)\n",
        "\n",
        "        df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
        "\n",
        "        df = df[['date', 'q']].rename(columns={'q': 'flow'})\n",
        "\n",
        "        df.to_csv(ruta_salida, index=False, header=True)\n",
        "\n",
        "print(\"IHA files processed successfully.\")"
      ],
      "id": "fba94450",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extra files and folders for sorting results\n",
        "\n",
        "4.4.1. List of folders to create:\n",
        "\n",
        "-   Folder 5 stores the station-specific folders generated by Climpact.\n",
        "\n",
        "-   Folder 6 contains customized plots based on Climpact results.\n",
        "\n",
        "-   Folder 9 stores the Excel files generated by IHA for each station, along with their processed CSV versions.\n",
        "\n",
        "-   Folder 10 includes the figures created from IHA data.\n",
        "\n",
        "-   Folder 11 contains text files intended to be imported into QGIS for georeferencing."
      ],
      "id": "73bd40c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ruta_base = os.path.join('Hidroclima', 'Base de datos')\n",
        "\n",
        "carpetas = [\n",
        "    '5.Climpact',\n",
        "    '6.Figuras_Climpact',\n",
        "    '9.IHA',\n",
        "    '10.Figuras_IHA',\n",
        "    '11.Georreferenciación'\n",
        "]\n",
        "\n",
        "for carpeta in carpetas:\n",
        "    ruta_carpeta = os.path.join(ruta_base, carpeta)\n",
        "    os.makedirs(ruta_carpeta, exist_ok=True)\n",
        "    print(f\"Folder created: {ruta_carpeta}\")\n",
        "\n",
        "print(\"All necessary folders for the results have been created successfully.\")"
      ],
      "id": "20403022",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.4.2. Georeference the stations in the study basin"
      ],
      "id": "08100a9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-polar-11\n",
        "#| tbl-cap: Information by georeferenced station\n",
        "archivo_entrada = \"Hidroclima/Base de datos/CAMELS_CL_v202201/catchment_attributes.csv\"\n",
        "df = pd.read_csv(archivo_entrada)\n",
        "\n",
        "df_filtrado = df[(df['gauge_id'] >= 9400000) & (df['gauge_id'] <= 9499999)]\n",
        "\n",
        "df_filtrado = df_filtrado[['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'record_period_start', 'record_period_end']]\n",
        "\n",
        "directorio_salida = \"Hidroclima/Base de datos/11.Georreferenciación/Estaciones.csv\"\n",
        "\n",
        "df_filtrado.to_csv(directorio_salida, index=False)\n",
        "\n",
        "print(\"Filtered file saved as:\", directorio_salida)\n",
        "\n",
        "from IPython.display import display\n",
        "display(df_filtrado)"
      ],
      "id": "tbl-polar-11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation of figures for climate indicators script\n",
        "\n",
        "By default, Climpact generates figures for each calculated indicator at each station. However, the following script provides a general framework for creating customized figures.\n",
        "\n",
        "### General framework for customized figures\n",
        "\n",
        "5.1.1. Libraries."
      ],
      "id": "1fb0143c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from scipy.stats import linregress"
      ],
      "id": "18dcceb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.1.2. Base route and output route.\n",
        "\n",
        "In this case, the figure of the annual accumulated precipitation indicator is generated."
      ],
      "id": "f1fa4a36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Precipitation'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "contador = 0\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue"
      ],
      "id": "895d7b0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.1.3. Selecting the indicator to be graphed.\n",
        "\n",
        "In this case, the annual total precipitation indicator is used, identified as `prcptot` in the dataset and therefore with its corresponding station code followed by `_prcptot_ANN.csv` in its file name. To select a different indicator, simply use the name of the corresponding file that contains the desired variable, for example, `_txx_ANN.csv` and the `txx` variable for monthly maximum daily temperature."
      ],
      "id": "ea0fdb41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_prcptot_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'prcptot']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['prcptot'] != -99.9]\n",
        "        df.set_index('time', inplace=True)"
      ],
      "id": "34f7fee8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.1.4. Create the figure.\n",
        "\n",
        "The use of Sen's slope, the regression line, and the statistical values calculated by the Climpact software have not yet been systematically included in the figure."
      ],
      "id": "faf6cb32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-1\n",
        "#| fig-cap: Indicator of total accumulated precipitation at station 9434001\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['prcptot'], marker='D', linestyle='-', color='black', label='Annual precipitation')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['prcptot'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Annual precipitation\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Precipitation (mm)', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.1, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_prcptot_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "        \n",
        "        contador += 1"
      ],
      "id": "fig-polar-1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Suggested precipitation indicators and their figures\n",
        "\n",
        "5.2.1. Precipitation intensity (sdii).\n",
        "\n",
        "Annual total precipitation divided by the number of wet days (when total precipitation \\>= 1.0 mm) is now graphed for each station."
      ],
      "id": "7753ec99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-2\n",
        "#| fig-cap: Indicator of precipitation intensity at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Precipitation'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "contador = 0\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_sdii_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'sdii']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['sdii'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['sdii'], marker='D', linestyle='-', color='black', label='Precipitation intensity')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['sdii'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Precipitation intensity\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Precipitation (mm)', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.1, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_sdii_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2.2. Anual accumulated precipitation (prcptot).\n",
        "\n",
        "Annual sum of daily precipitation \\>= 1.0 mm is now graphed for each station. This was already calculated above."
      ],
      "id": "932c9da9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-3\n",
        "#| fig-cap: Indicator of total accumulated precipitation at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Precipitation'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "contador = 0\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_prcptot_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'prcptot']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['prcptot'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['prcptot'], marker='D', linestyle='-', color='black', label='Total Precipitation')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['prcptot'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Anual precipitation\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Precipitation (mm)', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.1, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_prcptot_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2.3. Consecutive dry days (cdd).\n",
        "\n",
        "Maximum annual number of consecutive dry days (when precipitation \\< 1.0 mm)."
      ],
      "id": "84531d54"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-4\n",
        "#| fig-cap: Indicator of consecutive dry days at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Precipitation'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_cdd_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'cdd']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['cdd'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['cdd'], marker='D', linestyle='-', color='black', label='Dry days')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['cdd'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Consecutive dry days\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Days', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.1, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_cdd_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2.4. Days with precipitation greater than 30mm (r30mm).\n",
        "\n",
        "Number of days when precipitation \\>= 30mm."
      ],
      "id": "86d7db66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-5\n",
        "#| fig-cap: Indicator of days with precipitation greater than 30 mm at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Precipitation'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_r30mm_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'r30mm']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['r30mm'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['r30mm'], marker='D', linestyle='-', color='black', label='Dry days')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['r30mm'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Precipitation >= 30mm\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Days', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.1, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_r30mm_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2.5. Standardised Precipitation Evapotranspiration Index (SPEI)\n",
        "\n",
        "This indicator estimates water balance using precipitation and temperature information. It provides a drought indicator. In this case, it is graphed on a 24-month scale. Due to the monthly scale of the indicator, the code has slight adjustments."
      ],
      "id": "91093dd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-6\n",
        "#| fig-cap: Indicator of Standardised Precipitation Evapotrasnpiration Index at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Precipitation'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_24month_spei_MON.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'spei']]\n",
        "\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y-%m')\n",
        "\n",
        "        df = df[df['spei'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        df_pos = df[df['spei'] >= 0]\n",
        "        df_neg = df[df['spei'] < 0]\n",
        "\n",
        "        plt.plot(df.index, df['spei'], linestyle='-', color='black', label='SPEI')\n",
        "\n",
        "        plt.plot(df_pos.index, df_pos['spei'], marker='o', linestyle='None', color='black', label='SPEI ≥ 0')\n",
        "\n",
        "        plt.plot(df_neg.index, df_neg['spei'], marker='o', linestyle='None', color='gray', label='SPEI < 0')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['spei'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - SPEI compared to the last 24 months\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('SPEI', fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.1, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_spei24_MON_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Suggested temperature indicators and their figures\n",
        "\n",
        "5.3.1. Annual mean daily minimum temperature (tnm)."
      ],
      "id": "fd423f9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-7\n",
        "#| fig-cap: Indicator of annual mean daily minimum temperature at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Temperature'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_tnm_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'tnm']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['tnm'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['tnm'], marker='D', linestyle='-', color='black', label='Temperature')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['tnm'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Annual mean daily minimum temperature\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Temperature (ºC)', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.4, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_tnm_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3.2. Annual mean daily maximum temperature (txm)."
      ],
      "id": "8926b870"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-8\n",
        "#| fig-cap: Indicator of annual mean daily maximum temperature at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Temperature'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_txm_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'txm']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['txm'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['txm'], marker='D', linestyle='-', color='black', label='Temperature')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['txm'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Annual mean daily maximum temperature\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Temperature (ºC)', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.4, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_txm_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3.3. Annual warmest daily maximum temperature (txx)."
      ],
      "id": "d68a1e69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-9\n",
        "#| fig-cap: Indicator of annual warmest daily maximum temperature at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Temperature'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_txx_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'txx']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['txx'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['txx'], marker='D', linestyle='-', color='black', label='Temperature')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['txx'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Annual warmest daily maximum temperature\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Temperature (ºC)', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.4, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_txx_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3.4. Annual number of days when minimum temperature \\< 0ºC (id)."
      ],
      "id": "bb772b17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-10\n",
        "#| fig-cap: Indicator of annual numbery of days when minimum temperature is lower than 0ºC at station 9423001\n",
        "ruta_base = 'Hidroclima/Base de datos/5.Climpact'\n",
        "ruta_salida_base = 'Hidroclima/Base de datos/6.Figuras_Climpact/Temperature'\n",
        "os.makedirs(ruta_salida_base, exist_ok=True)\n",
        "\n",
        "subcarpetas = [os.path.join(ruta_base, carpeta) for carpeta in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, carpeta))]\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for subcarpeta in subcarpetas:\n",
        "    carpeta_indices = os.path.join(subcarpeta, 'indices')\n",
        "    if not os.path.exists(carpeta_indices):\n",
        "        continue\n",
        "\n",
        "    archivos = [f for f in os.listdir(carpeta_indices) if f.endswith('_fd_ANN.csv')]\n",
        "    for archivo in archivos:\n",
        "        ruta_archivo = os.path.join(carpeta_indices, archivo)\n",
        "\n",
        "        df = pd.read_csv(ruta_archivo, skiprows=6)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df[['time', 'fd']]\n",
        "        df['time'] = pd.to_datetime(df['time'], format='%Y')\n",
        "        df = df[df['fd'] != -99.9]\n",
        "        df.set_index('time', inplace=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(df.index, df['fd'], marker='D', linestyle='-', color='black', label='Days T<0ºC')\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(mdates.date2num(df.index), df['fd'])\n",
        "        plt.plot(df.index, intercept + slope * mdates.date2num(df.index), linestyle='--', color='black', label='Regression line')\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "        titulo = f\"{archivo.split('_')[0]} Station - Days when minimum temperature < 0ºC\"\n",
        "        #plt.title(titulo, fontsize=24, fontname='Times New Roman')\n",
        "        plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "        plt.ylabel('Days', fontsize=24, fontname='Times New Roman')\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.xticks(rotation=45, fontsize=20, fontname='Times New Roman')\n",
        "        plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "\n",
        "        info_text = (\n",
        "            f\"Slope: {slope:.2f}\\n\"\n",
        "            f\"Confidence interval: [{intercept - 1.96 * std_err:.2f}, {intercept + 1.96 * std_err:.2f}]\\n\"\n",
        "            f\"p-value: {p_value:.3f}\"\n",
        "        )\n",
        "        plt.text(1.1, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top',\n",
        "                 bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.grid(False)\n",
        "        plt.legend(bbox_to_anchor=(1.4, 0.4), fontsize=14, frameon=False)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        estacion_codigo = archivo.split('_')[0]\n",
        "        ruta_salida_estacion = os.path.join(ruta_salida_base, estacion_codigo)\n",
        "        os.makedirs(ruta_salida_estacion, exist_ok=True)\n",
        "        ruta_salida = os.path.join(ruta_salida_estacion, f\"{estacion_codigo}_fd_ANN_plot.png\")\n",
        "\n",
        "        plt.savefig(ruta_salida)\n",
        "\n",
        "        if contador == 0:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "        contador += 1"
      ],
      "id": "fig-polar-10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation of figures for hydrologic indicators script\n",
        "\n",
        "### Libraries and workspace."
      ],
      "id": "05cedd12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_directory = 'Hidroclima/Base de datos/9.IHA'\n",
        "\n",
        "sheets_to_convert = ['ann', 'sco', 'lsq', 'pct', 'daily efcs', 'fdc']\n",
        "\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
        "        filepath = os.path.join(input_directory, filename)\n",
        "        excel_file = pd.ExcelFile(filepath)\n",
        "        \n",
        "        station_code = os.path.splitext(filename)[0]\n",
        "        \n",
        "        station_output_directory = os.path.join(input_directory, 'csv', station_code)\n",
        "        \n",
        "        if not os.path.exists(station_output_directory):\n",
        "            os.makedirs(station_output_directory)\n",
        "        \n",
        "        for sheet in sheets_to_convert:\n",
        "            if sheet in excel_file.sheet_names:\n",
        "                df = pd.read_excel(filepath, sheet_name=sheet)\n",
        "                \n",
        "                output_filename = f\"{sheet}_{station_code}.csv\"\n",
        "                output_filepath = os.path.join(station_output_directory, output_filename)\n",
        "                \n",
        "                df.to_csv(output_filepath, index=False)\n",
        "                \n",
        "print(\"ann, sco, fdc, pct, lsq and daily efcs files saved in its folder.\")"
      ],
      "id": "89900e7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streamflow figures\n",
        "\n",
        "6.2.1. Simple monthly median flow for each station\n",
        "\n",
        "The order of the months in the `sco_` file may vary. If so, adjust the `meses_ingles` variable accordingly."
      ],
      "id": "870a8bd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-11\n",
        "#| fig-cap: Indicator of simple monthly median flow at station 9423001\n",
        "base_path = 'Hidroclima/Base de datos/9.IHA/csv'\n",
        "\n",
        "output_root_directory = 'Hidroclima/Base de datos/10.Figuras_IHA/Caudal'\n",
        "\n",
        "meses_ingles = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for file in files:\n",
        "        if file.startswith(\"sco_\") and file.endswith(\".csv\"):\n",
        "            sco_file = os.path.join(root, file)\n",
        "            \n",
        "            df = pd.read_csv(sco_file, skiprows=17, header=None)\n",
        "            df_mes = df.iloc[:12, :2]\n",
        "            df_mes.columns = ['Month', 'Median']\n",
        "            df_mes['Month'] = meses_ingles\n",
        "            df_mes.set_index('Month', inplace=True)\n",
        "\n",
        "            station_code = file.split('_')[1].split('.')[0]\n",
        "            output_dir = os.path.join(output_root_directory, station_code)\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.makedirs(output_dir)\n",
        "            \n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.plot(df_mes.index, df_mes['Median'], marker='.', linestyle='-', color='black', label='Median')\n",
        "            plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "            #plt.title(f'Median monthly flow - {station_code} Station', fontsize=24, fontname='Times New Roman')\n",
        "            plt.xlabel('Month', fontsize=24, fontname='Times New Roman')\n",
        "            plt.ylabel('Flow (m3/s)', fontsize=24, fontname='Times New Roman')\n",
        "            plt.xticks(fontsize=24, fontname='Times New Roman', rotation=45)\n",
        "            plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "            plt.grid(True, linestyle='-', linewidth=0.5, color='lightgray')\n",
        "            plt.legend(fontsize=14)\n",
        "            plt.grid(False)\n",
        "\n",
        "            output_file = os.path.join(output_dir, f'Median_monthly_flow_{station_code}.png')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_file, dpi=300)\n",
        "            \n",
        "            plt.savefig(ruta_salida)\n",
        "\n",
        "            if contador == 0:\n",
        "                plt.show()\n",
        "            else:\n",
        "                plt.close()\n",
        "\n",
        "            contador += 1\n",
        "            print(f'Figure saved: {output_file}')"
      ],
      "id": "fig-polar-11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.2.2. Boxplot flow for each month calculated from all recorded years for each station\n",
        "\n",
        "The language of the months or even the way they are written in the `ann_` file may vary. If so, adjust the `columns` variable and `id_vars` accordingly."
      ],
      "id": "fe114eb7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-12\n",
        "#| fig-cap: Boxplot of flow at station 9423001\n",
        "base_directory = 'Hidroclima/Base de datos/9.IHA/csv'\n",
        "output_root_directory = os.path.join('Hidroclima/Base de datos/10.Figuras_IHA', 'Caudal')\n",
        "if not os.path.exists(output_root_directory):\n",
        "    os.makedirs(output_root_directory)\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for station_folder in os.listdir(base_directory):\n",
        "    station_path = os.path.join(base_directory, station_folder)\n",
        "    if os.path.isdir(station_path):\n",
        "        station_output_directory = os.path.join(output_root_directory, station_folder)\n",
        "        if not os.path.exists(station_output_directory):\n",
        "            os.makedirs(station_output_directory)\n",
        "        \n",
        "        for filename in os.listdir(station_path):\n",
        "            if filename.startswith('ann_') and filename.endswith('.csv'):\n",
        "                input_file = os.path.join(station_path, filename)\n",
        "                \n",
        "                df = pd.read_csv(input_file, skiprows=4)\n",
        "                \n",
        "                columns = ['Year', 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "                df = df[columns]\n",
        "\n",
        "                df_melted = df.melt(id_vars=['Year'], var_name='Month', value_name='Value')\n",
        "                \n",
        "                month_translation = {\n",
        "                    'January': 'January', 'February': 'February', 'March': 'March', 'April': 'April', \n",
        "                    'May': 'May', 'June': 'June', 'July': 'July', 'August': 'August', \n",
        "                    'September': 'September', 'October': 'October', 'November': 'November', \n",
        "                    'December': 'December'\n",
        "                }\n",
        "\n",
        "                df_melted['Month'] = df_melted['Month'].map(month_translation)\n",
        "                \n",
        "                plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
        "                plt.rcParams[\"font.size\"] = 12\n",
        "                plt.rcParams[\"axes.titlesize\"] = plt.rcParams[\"font.size\"] + 4\n",
        "                plt.rcParams[\"axes.labelsize\"] = plt.rcParams[\"font.size\"] + 4\n",
        "                plt.rcParams[\"xtick.labelsize\"] = plt.rcParams[\"font.size\"] + 4\n",
        "                plt.rcParams[\"ytick.labelsize\"] = plt.rcParams[\"font.size\"] + 4\n",
        "                \n",
        "                plt.figure(figsize=(12, 6))\n",
        "                sns.boxplot(\n",
        "                    x='Month', y='Value', data=df_melted,\n",
        "                    order=['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
        "                           'August', 'September', 'October', 'November', 'December'],\n",
        "                    color='gray'\n",
        "                )\n",
        "                #plt.title(f'Distribution of the median monthly flow - {station_folder} Station')\n",
        "                plt.xlabel('Month')\n",
        "                plt.ylabel('Flow (m3/s)')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.tight_layout()\n",
        "                \n",
        "                output_plot_path = os.path.join(station_output_directory, f'BOXPLOT_{filename[:-4]}.png')\n",
        "                plt.savefig(output_plot_path, dpi=300)\n",
        "                \n",
        "                if contador == 0:\n",
        "                    plt.show()\n",
        "                else:\n",
        "                    plt.close()\n",
        "\n",
        "                contador += 1\n",
        "                \n",
        "                print(f'Boxplot saved: {output_plot_path}')"
      ],
      "id": "fig-polar-12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.2.3. Figure with the percentiles of monthly flows for all years evaluated by station.\n",
        "\n",
        "The order of the months in the `pct_` file may vary. If so, adjust the `meses_ingles` variable accordingly."
      ],
      "id": "e71d837e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-13\n",
        "#| fig-cap: Percentiles of monthly flows at station 9423001\n",
        "base_path = 'Hidroclima/Base de datos/9.IHA/csv'\n",
        "output_root_directory = 'Hidroclima/Base de datos/10.Figuras_IHA/Caudal'\n",
        "\n",
        "contador = 0\n",
        "\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for file in files:\n",
        "        if file.startswith(\"pct_\") and file.endswith(\".csv\"):\n",
        "            pct_file = os.path.join(root, file)\n",
        "            \n",
        "            station_code = os.path.basename(root)\n",
        "            \n",
        "            station_output_directory = os.path.join(output_root_directory, station_code)\n",
        "            \n",
        "            if not os.path.exists(station_output_directory):\n",
        "                print(f\"Error: The output directory for station {station_code} does not exist.\")\n",
        "                continue\n",
        "            \n",
        "            df = pd.read_csv(pct_file, skiprows=8, header=None) \n",
        "            df_mes = df.iloc[:12, :6]\n",
        "            df_mes.columns = ['Month', '10%', '25%', '50%', '75%', '90%']\n",
        "            \n",
        "            meses_ingles = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "            \n",
        "            df_mes['Month'] = meses_ingles\n",
        "            df_mes.set_index('Month', inplace=True)\n",
        "            \n",
        "            plt.figure(figsize=(12, 8))\n",
        "            linestyles = ['-', '--', '-.', ':', (0, (1, 10))]\n",
        "            marcadores = ['o', 's', '^', 'D', '*']\n",
        "            \n",
        "            for percentil, linestyle, marcador in zip(['10%', '25%', '50%', '75%', '90%'], linestyles, marcadores):\n",
        "                plt.plot(df_mes.index, df_mes[percentil], marker=marcador, linestyle=linestyle, label=percentil, color='black')\n",
        "            \n",
        "            plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "            #plt.title(f'Percentiles of monthly flows for {station_code}', fontsize=24, fontname='Times New Roman')\n",
        "            plt.xlabel('Month', fontsize=24, fontname='Times New Roman')\n",
        "            plt.ylabel('Flow (m3/s)', fontsize=24, fontname='Times New Roman')\n",
        "            plt.xticks(fontsize=24, fontname='Times New Roman', rotation=45)\n",
        "            plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "            plt.legend(bbox_to_anchor=(1.35, 0.95), fontsize=14)\n",
        "            plt.grid(False)\n",
        "            plt.tight_layout()\n",
        "            \n",
        "            output_plot_path = os.path.join(station_output_directory, f'percentiles_{station_code}.png')\n",
        "            plt.savefig(output_plot_path, dpi=300)\n",
        "            \n",
        "            if contador == 0:\n",
        "                plt.show()\n",
        "            else:\n",
        "                plt.close()\n",
        "            \n",
        "            contador += 1\n",
        "            \n",
        "            print(f'Figure saved: {output_plot_path}')"
      ],
      "id": "fig-polar-13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.2.4. Graphs for each station with the probability of exceedance for each month and at an annual level.\n",
        "\n",
        "The language of the months or even the way they are written in the `fdc_` file may vary. If so, adjust the `meses` variable accordingly."
      ],
      "id": "043632fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-14\n",
        "#| fig-cap: Flow Duration Curve for January at station 9423001\n",
        "base_path = 'Hidroclima/Base de datos/9.IHA/csv'\n",
        "output_root_directory = 'Hidroclima/Base de datos/10.Figuras_IHA/Caudal'\n",
        "\n",
        "contador = 0\n",
        "\n",
        "meses = {\n",
        "    'Annual': 'Annual', 'January': 'January', 'February': 'February', 'March': 'March',\n",
        "    'April': 'April', 'May': 'May', 'June': 'June', 'July': 'July', \n",
        "    'August': 'August', 'September': 'September', 'October': 'October', \n",
        "    'November': 'November', 'December': 'December'\n",
        "}\n",
        "\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for file in files:\n",
        "        if file.startswith(\"fdc_\") and file.endswith(\".csv\"):\n",
        "            fdc_file = os.path.join(root, file)\n",
        "            \n",
        "            df_headers = pd.read_csv(fdc_file, skiprows=9, nrows=1, header=None)\n",
        "            df_data = pd.read_csv(fdc_file, skiprows=11, header=None)\n",
        "            \n",
        "            col_indices = {}\n",
        "            for col_idx, col_name in enumerate(df_headers.iloc[0]):\n",
        "                if isinstance(col_name, str) and col_name.strip() in meses:\n",
        "                    col_indices[meses[col_name.strip()]] = col_idx\n",
        "            \n",
        "            station_code = file.split('_')[1].split('.')[0]\n",
        "            output_dir = os.path.join(output_root_directory, station_code)\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.makedirs(output_dir)\n",
        "            \n",
        "            for mes, mes_label in meses.items():\n",
        "                if mes_label in col_indices:\n",
        "                    x_col = col_indices[mes_label] + 1\n",
        "                    y_col = col_indices[mes_label] \n",
        "\n",
        "                    data = df_data.iloc[:, [y_col, x_col]].dropna()\n",
        "                    data.columns = ['Caudal', 'Probabilidad']\n",
        "                    data['Probabilidad'] = data['Probabilidad']\n",
        "                    \n",
        "                    plt.figure(figsize=(12, 8))\n",
        "                    plt.plot(data['Probabilidad'], data['Caudal'], marker='o', linestyle='-', color='black')\n",
        "                    #plt.title(f'Flow Duration Curve - {mes_label} ({station_code})', fontsize=16, fontname='Times New Roman')\n",
        "                    plt.xlabel('Probability of Exceedance (%)', fontsize=14, fontname='Times New Roman')\n",
        "                    plt.ylabel('Flow (m³/s)', fontsize=14, fontname='Times New Roman')\n",
        "                    plt.grid(False)\n",
        "                    plt.tight_layout()\n",
        "                    \n",
        "                    output_file = os.path.join(output_dir, f'FDC_{mes_label}_{station_code}.png')\n",
        "                    plt.savefig(output_file, dpi=300)\n",
        "                    \n",
        "                    if contador == 1:\n",
        "                        plt.show()\n",
        "                    else:\n",
        "                        plt.close()\n",
        "                        \n",
        "                    contador += 1\n",
        "                    \n",
        "print('Flow Duration Curve generated for each month of each station')"
      ],
      "id": "fig-polar-14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.2.5. Figure of the streamflow of all stations present in the basin\n",
        "\n",
        "The order of the months in the `sco_` file may vary. If so, adjust the `meses_ingles` variable accordingly."
      ],
      "id": "dc7e15af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-15\n",
        "#| fig-cap: Median monthly flow by station\n",
        "base_path = 'Hidroclima/Base de datos/9.IHA/csv'\n",
        "save_path = 'Hidroclima/Base de datos/10.Figuras_IHA/Caudal'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "linestyles = ['-', '--', '-.', ':', (0, (1, 10)), (0, (5, 10)), (0, (3, 5)), (0, (3, 1)), (0, (5, 3)), (0, (1, 1)), (0, (5, 1)), (0, (1, 5))]\n",
        "colores = ['black', 'darkgray', 'gray', 'lightgray', 'black', 'dimgray', 'gray', 'dimgray', 'darkgray', 'black', 'dimgray', 'black']\n",
        "plt.figure(figsize=(12, 8))\n",
        "line_counter = 0\n",
        "\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for file in files:\n",
        "        if file.startswith(\"sco_\") and file.endswith(\".csv\"):\n",
        "            sco_file = os.path.join(root, file)\n",
        "            \n",
        "            nombre_archivo = os.path.basename(sco_file)\n",
        "            codigo_estacion = nombre_archivo.split('_')[1].split('.')[0]\n",
        "            \n",
        "            df = pd.read_csv(sco_file, skiprows=17, header=None)\n",
        "            df_mes = df.iloc[:12, :2]\n",
        "            df_mes.columns = ['Month', 'Median']\n",
        "            \n",
        "            meses_ingles = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "            df_mes['Month'] = meses_ingles\n",
        "            df_mes.set_index('Month', inplace=True)\n",
        "            \n",
        "            linestyle = linestyles[line_counter % len(linestyles)]\n",
        "            color = colores[line_counter % len(colores)]\n",
        "            plt.plot(df_mes.index, df_mes['Median'], linestyle=linestyle, color=color, label=f'Station {codigo_estacion}')\n",
        "            line_counter += 1\n",
        "\n",
        "plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "#plt.title('Median monthly flow by station', fontsize=24, fontname='Times New Roman')\n",
        "plt.xlabel('Month', fontsize=24, fontname='Times New Roman')\n",
        "plt.ylabel('Flow (m3/s)', fontsize=24, fontname='Times New Roman')\n",
        "plt.xticks(fontsize=24, fontname='Times New Roman', rotation=45)\n",
        "plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "plt.grid(True, linestyle='-', linewidth=0.5, color='black')\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(False)\n",
        "\n",
        "save_file = os.path.join(save_path, 'qmonthly_TOTstations.png')\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_file, dpi=300)\n",
        "plt.show()"
      ],
      "id": "fig-polar-15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time series figures\n",
        "\n",
        "The language of the months, indicator and column of \"year\" or even the way they are written in the `ann_` and `sco_` file may vary.\n",
        "\n",
        "If so, adjust:\n",
        "\n",
        "-   `indicator_names` variable\n",
        "\n",
        "-   the names for the regression in row `lsq_selected = lsq_data.loc[indicator['lsq_row'], ['Slope', 'YInt', 'Sigma', 'Corr', 'PValue', 'FStat', 'R2']]`\n",
        "\n",
        "-   the `skiprows` variable in `sco_data` with the number of the row where the list of the respective indicator begins\n",
        "\n",
        "-   the `slope`, `intercept` and `r_squared` variables\n",
        "\n",
        "6.3.1. Median flow in July over time.\n",
        "\n",
        "In this case, the time series of the median flow in July was selected, as it corresponds to the month with the highest flow in the study basin.\n",
        "\n",
        "6.3.1.1. Reading input files and extracting specific data related to the selected month."
      ],
      "id": "b2badb27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_folder_path = 'Hidroclima/Base de datos/9.IHA/csv/'\n",
        "output_folder_path = 'Hidroclima/Base de datos/10.Figuras_IHA/IHA anuales/'\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "indicator_names = {\n",
        "    'lsq_row': 'July',              \n",
        "    'ann_column': 'July',           \n",
        "    'sco_row': 'July',              \n",
        "    'plot_label': 'July'            \n",
        "}\n",
        "\n",
        "def extract_data(lsq_file, ann_file, sco_file, indicator):\n",
        "    lsq_data = pd.read_csv(lsq_file, skiprows=3)\n",
        "    lsq_data.set_index(lsq_data.columns[0], inplace=True)\n",
        "    try:\n",
        "        lsq_selected = lsq_data.loc[indicator['lsq_row'], ['Slope', 'YInt', 'Sigma', 'Corr', 'PValue', 'FStat', 'R2']]\n",
        "    except KeyError:\n",
        "        print(f\"Indicator '{indicator['lsq_row']}' not found in lsq_file.\")\n",
        "        return None, None, None\n",
        "\n",
        "    ann_data = pd.read_csv(ann_file, skiprows=4)\n",
        "    ann_data.columns = ann_data.columns.str.strip()\n",
        "    if indicator['ann_column'] not in ann_data.columns:\n",
        "        print(f\"Column '{indicator['ann_column']}' not found in ann_file.\")\n",
        "        return None, None, None\n",
        "    ann_data = ann_data[['Year', indicator['ann_column']]].dropna()\n",
        "\n",
        "    sco_data = pd.read_csv(sco_file, skiprows=17, nrows=13, header=None, index_col=0)\n",
        "    sco_data.index.name = 'Parameter'\n",
        "    sco_data.columns = ['Mediana', 'Coef. Disper.']\n",
        "    if indicator['sco_row'] not in sco_data.index:\n",
        "        print(f\"Indicator '{indicator['sco_row']}' not found in sco_file.\")\n",
        "        return None, None, None\n",
        "    sco_selected = sco_data.loc[[indicator['sco_row']]]\n",
        "\n",
        "    return lsq_selected, ann_data, sco_selected"
      ],
      "id": "8bcf913c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.1.2. Saving the extracted data in a single `.csv` file."
      ],
      "id": "8e0c36db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_to_csv(lsq_data, ann_data, sco_data, output_file):\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        \n",
        "        writer.writerow(['Variable', 'Valor'])\n",
        "        for idx, value in lsq_data.items():\n",
        "            writer.writerow([idx, value])\n",
        "        \n",
        "        writer.writerow(['Year', indicator_names['ann_column']])\n",
        "        for _, row in ann_data.iterrows():\n",
        "            writer.writerow([int(row['Year']), row[indicator_names['ann_column']]])\n",
        "        \n",
        "        writer.writerow(['Mediana', sco_data['Mediana'].values[0]])\n",
        "        writer.writerow(['Coef. Disper.', sco_data['Coef. Disper.'].values[0]])"
      ],
      "id": "063c3127",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.1.3. Graph format."
      ],
      "id": "75dead24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_data(df, slope, intercept, r_squared, station_code, plot_label):\n",
        "    df_anios = df[(df['Variable'].str.isnumeric()) & (df['Valor'].notna())].copy()\n",
        "    df_anios['Variable'] = pd.to_numeric(df_anios['Variable'], errors='coerce')\n",
        "    df_anios['Valor'] = pd.to_numeric(df_anios['Valor'], errors='coerce')\n",
        "    df_anios = df_anios.dropna()\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(df_anios['Variable'], df_anios['Valor'], marker='o', linestyle='None', color='black', label=plot_label)\n",
        "    \n",
        "    for i in range(1, len(df_anios)):\n",
        "        if df_anios['Variable'].iloc[i] - df_anios['Variable'].iloc[i-1] == 1:\n",
        "            plt.plot(df_anios['Variable'].iloc[i-1:i+1], df_anios['Valor'].iloc[i-1:i+1], linestyle='-', color='black')\n",
        "\n",
        "    line = slope * df_anios['Variable'] + intercept\n",
        "    plt.plot(df_anios['Variable'], line, linestyle='--', color='black', label='Regression')\n",
        "\n",
        "    plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "    #plt.title(f'{plot_label} ({station_code})', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "    plt.ylabel('Flow (m3/s)', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.legend(bbox_to_anchor=(1.35, 0.95), fontsize=14, frameon=False)\n",
        "    plt.grid(False)\n",
        "\n",
        "    info_text = f'Slope: {slope:.2f}\\nR-Squared: {r_squared:.2f}'\n",
        "    plt.text(1.15, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14,\n",
        "             verticalalignment='top', horizontalalignment='center',\n",
        "             bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "    station_folder = os.path.join(output_folder_path, station_code)\n",
        "    os.makedirs(station_folder, exist_ok=True)\n",
        "\n",
        "    graph_filename = os.path.join(station_folder, f\"{plot_label.lower().replace(' ', '_')}_{station_code}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(graph_filename)\n",
        "\n",
        "    print(f\"The CSV file and figure for station {station_code} has been saved in {graph_filename}\")"
      ],
      "id": "eb37ebdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.1.4. Graph generation."
      ],
      "id": "ffa8bae4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-16\n",
        "#| fig-cap: Median flow in July over time at station 9423001\n",
        "def process_all_stations(base_folder_path, output_folder_path, indicator):\n",
        "    station_folders = [f for f in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, f))]\n",
        "    contador = 0\n",
        "    \n",
        "    for station_code in station_folders:\n",
        "        lsq_file = os.path.join(base_folder_path, station_code, f'lsq_{station_code}.csv')\n",
        "        ann_file = os.path.join(base_folder_path, station_code, f'ann_{station_code}.csv')\n",
        "        sco_file = os.path.join(base_folder_path, station_code, f'sco_{station_code}.csv')\n",
        "\n",
        "        if not all(os.path.exists(f) for f in [lsq_file, ann_file, sco_file]):\n",
        "            print(f'Missing files for station {station_code}. Skipping station.')\n",
        "            continue\n",
        "\n",
        "        lsq_data, ann_data, sco_data = extract_data(lsq_file, ann_file, sco_file, indicator)\n",
        "        if lsq_data is None or ann_data is None or sco_data is None:\n",
        "            continue\n",
        "\n",
        "        output_file = os.path.join(output_folder_path, f\"{indicator['ann_column'].lower()}_{station_code}.csv\")\n",
        "        save_to_csv(lsq_data, ann_data, sco_data, output_file)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(output_file, names=['Variable', 'Valor'], skiprows=1)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {output_file}')\n",
        "            continue\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        try:\n",
        "            slope = float(df[df['Variable'] == 'Slope']['Valor'].values[0])\n",
        "            intercept = float(df[df['Variable'] == 'YInt']['Valor'].values[0])\n",
        "            r_squared = float(df[df['Variable'] == 'R2']['Valor'].values[0])\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f'Error extracting regression values: {e}')\n",
        "            continue\n",
        "\n",
        "        plot_data(df, slope, intercept, r_squared, station_code, indicator['plot_label'])\n",
        "        \n",
        "        if contador == 0:\n",
        "           plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "    \n",
        "        contador += 1 \n",
        "\n",
        "process_all_stations(base_folder_path, output_folder_path, indicator_names)"
      ],
      "id": "fig-polar-16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.2. High flow pulses duration by station\n",
        "\n",
        "6.3.2.1. Reading input files and extracting specific data related to the selected indicator."
      ],
      "id": "0a1ebe2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "indicator_names = {\n",
        "    'lsq_row': 'High pulse duration',              \n",
        "    'ann_column': 'Hi pulse L',           \n",
        "    'sco_row': 'High pulse duration',              \n",
        "    'plot_label': 'High pulse duration'            \n",
        "}\n",
        "\n",
        "def extract_data(lsq_file, ann_file, sco_file, indicator):\n",
        "    lsq_data = pd.read_csv(lsq_file, skiprows=3)\n",
        "    lsq_data.set_index(lsq_data.columns[0], inplace=True)\n",
        "    try:\n",
        "        lsq_selected = lsq_data.loc[indicator['lsq_row'], ['Slope', 'YInt', 'Sigma', 'Corr', 'PValue', 'FStat', 'R2']]\n",
        "    except KeyError:\n",
        "        print(f\"Indicator '{indicator['lsq_row']}' not found in lsq_file.\")\n",
        "        return None, None, None\n",
        "\n",
        "    ann_data = pd.read_csv(ann_file, skiprows=4)\n",
        "    ann_data.columns = ann_data.columns.str.strip()\n",
        "    if indicator['ann_column'] not in ann_data.columns:\n",
        "        print(f\"Column '{indicator['ann_column']}' not found in ann_file.\")\n",
        "        return None, None, None\n",
        "    ann_data = ann_data[['Year', indicator['ann_column']]].dropna()\n",
        "\n",
        "    sco_data = pd.read_csv(sco_file, skiprows=48, nrows=13, header=None, index_col=0)\n",
        "    sco_data.index.name = 'Parameter'\n",
        "    sco_data.columns = ['Mediana', 'Coef. Disper.']\n",
        "    if indicator['sco_row'] not in sco_data.index:\n",
        "        print(f\"Indicator '{indicator['sco_row']}' not found in sco_file.\")\n",
        "        return None, None, None\n",
        "    sco_selected = sco_data.loc[[indicator['sco_row']]]\n",
        "\n",
        "    return lsq_selected, ann_data, sco_selected"
      ],
      "id": "749961c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.2.2. Saving the extracted data in a single `.csv` file."
      ],
      "id": "a55bedfa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_to_csv(lsq_data, ann_data, sco_data, output_file):\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        \n",
        "        writer.writerow(['Variable', 'Valor'])\n",
        "        for idx, value in lsq_data.items():\n",
        "            writer.writerow([idx, value])\n",
        "        \n",
        "        writer.writerow(['Year', indicator_names['ann_column']])\n",
        "        for _, row in ann_data.iterrows():\n",
        "            writer.writerow([int(row['Year']), row[indicator_names['ann_column']]])\n",
        "        \n",
        "        writer.writerow(['Mediana', sco_data['Mediana'].values[0]])\n",
        "        writer.writerow(['Coef. Disper.', sco_data['Coef. Disper.'].values[0]])"
      ],
      "id": "2d12f30e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.2.3. Graph format."
      ],
      "id": "df37172e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_data(df, slope, intercept, r_squared, station_code, plot_label):\n",
        "    df_anios = df[(df['Variable'].str.isnumeric()) & (df['Valor'].notna())].copy()\n",
        "    df_anios['Variable'] = pd.to_numeric(df_anios['Variable'], errors='coerce')\n",
        "    df_anios['Valor'] = pd.to_numeric(df_anios['Valor'], errors='coerce')\n",
        "    df_anios = df_anios.dropna()\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(df_anios['Variable'], df_anios['Valor'], marker='o', linestyle='None', color='black', label=plot_label)\n",
        "    \n",
        "    for i in range(1, len(df_anios)):\n",
        "        if df_anios['Variable'].iloc[i] - df_anios['Variable'].iloc[i-1] == 1:\n",
        "            plt.plot(df_anios['Variable'].iloc[i-1:i+1], df_anios['Valor'].iloc[i-1:i+1], linestyle='-', color='black')\n",
        "\n",
        "    line = slope * df_anios['Variable'] + intercept\n",
        "    plt.plot(df_anios['Variable'], line, linestyle='--', color='black', label='Regression')\n",
        "\n",
        "    plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "    #plt.title(f'{plot_label} ({station_code})', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "    plt.ylabel('Days', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.legend(bbox_to_anchor=(1.35, 0.95), fontsize=14, frameon=False)\n",
        "    plt.grid(False)\n",
        "\n",
        "    info_text = f'Slope: {slope:.2f}\\nR-Squared: {r_squared:.2f}'\n",
        "    plt.text(1.15, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14,\n",
        "             verticalalignment='top', horizontalalignment='center',\n",
        "             bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "    station_folder = os.path.join(output_folder_path, station_code)\n",
        "    os.makedirs(station_folder, exist_ok=True)\n",
        "\n",
        "    graph_filename = os.path.join(station_folder, f\"{plot_label.lower().replace(' ', '_')}_{station_code}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(graph_filename)\n",
        "\n",
        "    print(f\"The CSV files and figure for station {station_code} has been saved in {graph_filename}\")"
      ],
      "id": "765a7ed7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.2.4. Graph generation."
      ],
      "id": "a267ad30"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-17\n",
        "#| fig-cap: High flow pulses duration over time at station 9423001\n",
        "def process_all_stations(base_folder_path, output_folder_path, indicator):\n",
        "    station_folders = [f for f in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, f))]\n",
        "    contador = 0\n",
        "    \n",
        "    for station_code in station_folders:\n",
        "        lsq_file = os.path.join(base_folder_path, station_code, f'lsq_{station_code}.csv')\n",
        "        ann_file = os.path.join(base_folder_path, station_code, f'ann_{station_code}.csv')\n",
        "        sco_file = os.path.join(base_folder_path, station_code, f'sco_{station_code}.csv')\n",
        "\n",
        "        if not all(os.path.exists(f) for f in [lsq_file, ann_file, sco_file]):\n",
        "            print(f'Missing files for station {station_code}. Skipping station.')\n",
        "            continue\n",
        "\n",
        "        lsq_data, ann_data, sco_data = extract_data(lsq_file, ann_file, sco_file, indicator)\n",
        "        if lsq_data is None or ann_data is None or sco_data is None:\n",
        "            continue\n",
        "\n",
        "        output_file = os.path.join(output_folder_path, f\"{indicator['ann_column'].lower()}_{station_code}.csv\")\n",
        "        save_to_csv(lsq_data, ann_data, sco_data, output_file)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(output_file, names=['Variable', 'Valor'], skiprows=1)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {output_file}')\n",
        "            continue\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        try:\n",
        "            slope = float(df[df['Variable'] == 'Slope']['Valor'].values[0])\n",
        "            intercept = float(df[df['Variable'] == 'YInt']['Valor'].values[0])\n",
        "            r_squared = float(df[df['Variable'] == 'R2']['Valor'].values[0])\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f'Error extracting regression values: {e}')\n",
        "            continue\n",
        "\n",
        "        plot_data(df, slope, intercept, r_squared, station_code, indicator['plot_label'])\n",
        "        \n",
        "        if contador == 0:\n",
        "           plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "    \n",
        "        contador += 1 \n",
        "\n",
        "process_all_stations(base_folder_path, output_folder_path, indicator_names)"
      ],
      "id": "fig-polar-17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.3. Base Flow Index by station\n",
        "\n",
        "6.3.3.1. Reading input files and extracting specific data related to the selected indicator."
      ],
      "id": "f0921df2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "indicator_names = {\n",
        "    'lsq_row': 'Base flow index',              \n",
        "    'ann_column': 'Base flow',           \n",
        "    'sco_row': 'Base flow index',              \n",
        "    'plot_label': 'Base flow index'            \n",
        "}\n",
        "\n",
        "def extract_data(lsq_file, ann_file, sco_file, indicator):\n",
        "    lsq_data = pd.read_csv(lsq_file, skiprows=3)\n",
        "    lsq_data.set_index(lsq_data.columns[0], inplace=True)\n",
        "    try:\n",
        "        lsq_selected = lsq_data.loc[indicator['lsq_row'], ['Slope', 'YInt', 'Sigma', 'Corr', 'PValue', 'FStat', 'R2']]\n",
        "    except KeyError:\n",
        "        print(f\"Indicator '{indicator['lsq_row']}' not found in lsq_file.\")\n",
        "        return None, None, None\n",
        "\n",
        "    ann_data = pd.read_csv(ann_file, skiprows=4)\n",
        "    ann_data.columns = ann_data.columns.str.strip()\n",
        "    if indicator['ann_column'] not in ann_data.columns:\n",
        "        print(f\"Column '{indicator['ann_column']}' not found in ann_file.\")\n",
        "        return None, None, None\n",
        "    ann_data = ann_data[['Year', indicator['ann_column']]].dropna()\n",
        "\n",
        "    sco_data = pd.read_csv(sco_file, skiprows=30, nrows=13, header=None, index_col=0)\n",
        "    sco_data.index.name = 'Parameter'\n",
        "    sco_data.columns = ['Mediana', 'Coef. Disper.']\n",
        "    if indicator['sco_row'] not in sco_data.index:\n",
        "        print(f\"Indicator '{indicator['sco_row']}' not found in sco_file.\")\n",
        "        return None, None, None\n",
        "    sco_selected = sco_data.loc[[indicator['sco_row']]]\n",
        "\n",
        "    return lsq_selected, ann_data, sco_selected"
      ],
      "id": "bd198761",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.3.2. Saving the extracted data in a single `.csv` file."
      ],
      "id": "224d9e21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_to_csv(lsq_data, ann_data, sco_data, output_file):\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        \n",
        "        writer.writerow(['Variable', 'Valor'])\n",
        "        for idx, value in lsq_data.items():\n",
        "            writer.writerow([idx, value])\n",
        "        \n",
        "        writer.writerow(['Year', indicator_names['ann_column']])\n",
        "        for _, row in ann_data.iterrows():\n",
        "            writer.writerow([int(row['Year']), row[indicator_names['ann_column']]])\n",
        "        \n",
        "        writer.writerow(['Mediana', sco_data['Mediana'].values[0]])\n",
        "        writer.writerow(['Coef. Disper.', sco_data['Coef. Disper.'].values[0]])"
      ],
      "id": "f193f90a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.3.3. Graph format."
      ],
      "id": "c0774e58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_data(df, slope, intercept, r_squared, station_code, plot_label):\n",
        "    df_anios = df[(df['Variable'].str.isnumeric()) & (df['Valor'].notna())].copy()\n",
        "    df_anios['Variable'] = pd.to_numeric(df_anios['Variable'], errors='coerce')\n",
        "    df_anios['Valor'] = pd.to_numeric(df_anios['Valor'], errors='coerce')\n",
        "    df_anios = df_anios.dropna()\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(df_anios['Variable'], df_anios['Valor'], marker='o', linestyle='None', color='black', label=plot_label)\n",
        "    \n",
        "    for i in range(1, len(df_anios)):\n",
        "        if df_anios['Variable'].iloc[i] - df_anios['Variable'].iloc[i-1] == 1:\n",
        "            plt.plot(df_anios['Variable'].iloc[i-1:i+1], df_anios['Valor'].iloc[i-1:i+1], linestyle='-', color='black')\n",
        "\n",
        "    line = slope * df_anios['Variable'] + intercept\n",
        "    plt.plot(df_anios['Variable'], line, linestyle='--', color='black', label='Regression')\n",
        "\n",
        "    plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "    #plt.title(f'{plot_label} ({station_code})', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "    plt.ylabel('BFI', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.legend(bbox_to_anchor=(1.35, 0.95), fontsize=14, frameon=False)\n",
        "    plt.grid(False)\n",
        "\n",
        "    info_text = f'Slope: {slope:.2f}\\nR-Squared: {r_squared:.2f}'\n",
        "    plt.text(1.15, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14,\n",
        "             verticalalignment='top', horizontalalignment='center',\n",
        "             bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "    station_folder = os.path.join(output_folder_path, station_code)\n",
        "    os.makedirs(station_folder, exist_ok=True)\n",
        "\n",
        "    graph_filename = os.path.join(station_folder, f\"{plot_label.lower().replace(' ', '_')}_{station_code}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(graph_filename)\n",
        "\n",
        "    print(f\"The CSV files and figure for station {station_code} has been saved in {graph_filename}\")"
      ],
      "id": "8e246b50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.3.4. Graph generation."
      ],
      "id": "14ddc9bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-18\n",
        "#| fig-cap: Base Flow Index over time at station 9423001\n",
        "def process_all_stations(base_folder_path, output_folder_path, indicator):\n",
        "    station_folders = [f for f in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, f))]\n",
        "    contador = 0\n",
        "    \n",
        "    for station_code in station_folders:\n",
        "        lsq_file = os.path.join(base_folder_path, station_code, f'lsq_{station_code}.csv')\n",
        "        ann_file = os.path.join(base_folder_path, station_code, f'ann_{station_code}.csv')\n",
        "        sco_file = os.path.join(base_folder_path, station_code, f'sco_{station_code}.csv')\n",
        "\n",
        "        if not all(os.path.exists(f) for f in [lsq_file, ann_file, sco_file]):\n",
        "            print(f'Missing files for station {station_code}. Skipping station.')\n",
        "            continue\n",
        "\n",
        "        lsq_data, ann_data, sco_data = extract_data(lsq_file, ann_file, sco_file, indicator)\n",
        "        if lsq_data is None or ann_data is None or sco_data is None:\n",
        "            continue\n",
        "\n",
        "        output_file = os.path.join(output_folder_path, f\"{indicator['ann_column'].lower()}_{station_code}.csv\")\n",
        "        save_to_csv(lsq_data, ann_data, sco_data, output_file)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(output_file, names=['Variable', 'Valor'], skiprows=1)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {output_file}')\n",
        "            continue\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        try:\n",
        "            slope = float(df[df['Variable'] == 'Slope']['Valor'].values[0])\n",
        "            intercept = float(df[df['Variable'] == 'YInt']['Valor'].values[0])\n",
        "            r_squared = float(df[df['Variable'] == 'R2']['Valor'].values[0])\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f'Error extracting regression values: {e}')\n",
        "            continue\n",
        "\n",
        "        plot_data(df, slope, intercept, r_squared, station_code, indicator['plot_label'])\n",
        "        \n",
        "        if contador == 0:\n",
        "           plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "    \n",
        "        contador += 1 \n",
        "\n",
        "process_all_stations(base_folder_path, output_folder_path, indicator_names)"
      ],
      "id": "fig-polar-18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.4. Small Floods Peaks over time\n",
        "\n",
        "The same process is repeated, adjusted to the specific indicator."
      ],
      "id": "4ea5d653"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-19\n",
        "#| fig-cap: Small Floods Peaks over time at station 9423001\n",
        "indicator_names = {\n",
        "    'lsq_row': 'Small Flood peak',              \n",
        "    'ann_column': 'Sfld1 peak',           \n",
        "    'sco_row': 'Small Flood peak',              \n",
        "    'plot_label': 'Small Flood peak'            \n",
        "}\n",
        "\n",
        "def extract_data(lsq_file, ann_file, sco_file, indicator):\n",
        "    lsq_data = pd.read_csv(lsq_file, skiprows=3)\n",
        "    lsq_data.set_index(lsq_data.columns[0], inplace=True)\n",
        "    try:\n",
        "        lsq_selected = lsq_data.loc[indicator['lsq_row'], ['Slope', 'YInt', 'Sigma', 'Corr', 'PValue', 'FStat', 'R2']]\n",
        "    except KeyError:\n",
        "        print(f\"Indicator '{indicator['lsq_row']}' not found in lsq_file.\")\n",
        "        return None, None, None\n",
        "\n",
        "    ann_data = pd.read_csv(ann_file, skiprows=4)\n",
        "    ann_data.columns = ann_data.columns.str.strip()\n",
        "    if indicator['ann_column'] not in ann_data.columns:\n",
        "        print(f\"Column '{indicator['ann_column']}' not found in ann_file.\")\n",
        "        return None, None, None\n",
        "    ann_data = ann_data[['Year', indicator['ann_column']]].dropna()\n",
        "\n",
        "    sco_data = pd.read_csv(sco_file, skiprows=80, nrows=13, header=None, index_col=0)\n",
        "    sco_data.index.name = 'Parameter'\n",
        "    sco_data.columns = ['Mediana', 'Coef. Disper.']\n",
        "    if indicator['sco_row'] not in sco_data.index:\n",
        "        print(f\"Indicator '{indicator['sco_row']}' not found in sco_file.\")\n",
        "        return None, None, None\n",
        "    sco_selected = sco_data.loc[[indicator['sco_row']]]\n",
        "\n",
        "    return lsq_selected, ann_data, sco_selected\n",
        "\n",
        "def save_to_csv(lsq_data, ann_data, sco_data, output_file):\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        \n",
        "        writer.writerow(['Variable', 'Valor'])\n",
        "        for idx, value in lsq_data.items():\n",
        "            writer.writerow([idx, value])\n",
        "        \n",
        "        writer.writerow(['Year', indicator_names['ann_column']])\n",
        "        for _, row in ann_data.iterrows():\n",
        "            writer.writerow([int(row['Year']), row[indicator_names['ann_column']]])\n",
        "        \n",
        "        writer.writerow(['Mediana', sco_data['Mediana'].values[0]])\n",
        "        writer.writerow(['Coef. Disper.', sco_data['Coef. Disper.'].values[0]])\n",
        "\n",
        "def plot_data(df, slope, intercept, r_squared, station_code, plot_label):\n",
        "    df_anios = df[(df['Variable'].str.isnumeric()) & (df['Valor'].notna())].copy()\n",
        "    df_anios['Variable'] = pd.to_numeric(df_anios['Variable'], errors='coerce')\n",
        "    df_anios['Valor'] = pd.to_numeric(df_anios['Valor'], errors='coerce')\n",
        "    df_anios = df_anios.dropna()\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(df_anios['Variable'], df_anios['Valor'], marker='o', linestyle='None', color='black', label=plot_label)\n",
        "    \n",
        "    for i in range(1, len(df_anios)):\n",
        "        if df_anios['Variable'].iloc[i] - df_anios['Variable'].iloc[i-1] == 1:\n",
        "            plt.plot(df_anios['Variable'].iloc[i-1:i+1], df_anios['Valor'].iloc[i-1:i+1], linestyle='-', color='black')\n",
        "\n",
        "    line = slope * df_anios['Variable'] + intercept\n",
        "    plt.plot(df_anios['Variable'], line, linestyle='--', color='black', label='Regression')\n",
        "\n",
        "    plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "    #plt.title(f'{plot_label} ({station_code})', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "    plt.ylabel('Flow (m3/s)', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.legend(bbox_to_anchor=(1.35, 0.95), fontsize=14, frameon=False)\n",
        "    plt.grid(False)\n",
        "\n",
        "    info_text = f'Slope: {slope:.2f}\\nR-Squared: {r_squared:.2f}'\n",
        "    plt.text(1.15, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14,\n",
        "             verticalalignment='top', horizontalalignment='center',\n",
        "             bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "    station_folder = os.path.join(output_folder_path, station_code)\n",
        "    os.makedirs(station_folder, exist_ok=True)\n",
        "\n",
        "    graph_filename = os.path.join(station_folder, f\"{plot_label.lower().replace(' ', '_')}_{station_code}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(graph_filename)\n",
        "\n",
        "    print(f\"The CSV files and figure for station {station_code} has been saved in {graph_filename}\")\n",
        "\n",
        "def process_all_stations(base_folder_path, output_folder_path, indicator):\n",
        "    station_folders = [f for f in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, f))]\n",
        "    contador = 0\n",
        "    \n",
        "    for station_code in station_folders:\n",
        "        lsq_file = os.path.join(base_folder_path, station_code, f'lsq_{station_code}.csv')\n",
        "        ann_file = os.path.join(base_folder_path, station_code, f'ann_{station_code}.csv')\n",
        "        sco_file = os.path.join(base_folder_path, station_code, f'sco_{station_code}.csv')\n",
        "\n",
        "        if not all(os.path.exists(f) for f in [lsq_file, ann_file, sco_file]):\n",
        "            print(f'Missing files for station {station_code}. Skipping station.')\n",
        "            continue\n",
        "\n",
        "        lsq_data, ann_data, sco_data = extract_data(lsq_file, ann_file, sco_file, indicator)\n",
        "        if lsq_data is None or ann_data is None or sco_data is None:\n",
        "            continue\n",
        "\n",
        "        output_file = os.path.join(output_folder_path, f\"{indicator['ann_column'].lower()}_{station_code}.csv\")\n",
        "        save_to_csv(lsq_data, ann_data, sco_data, output_file)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(output_file, names=['Variable', 'Valor'], skiprows=1)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {output_file}')\n",
        "            continue\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        try:\n",
        "            slope = float(df[df['Variable'] == 'Slope']['Valor'].values[0])\n",
        "            intercept = float(df[df['Variable'] == 'YInt']['Valor'].values[0])\n",
        "            r_squared = float(df[df['Variable'] == 'R2']['Valor'].values[0])\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f'Error extracting regression values: {e}')\n",
        "            continue\n",
        "\n",
        "        plot_data(df, slope, intercept, r_squared, station_code, indicator['plot_label'])\n",
        "        \n",
        "        if contador == 0:\n",
        "           plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "    \n",
        "        contador += 1 \n",
        "\n",
        "process_all_stations(base_folder_path, output_folder_path, indicator_names)"
      ],
      "id": "fig-polar-19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.4. Large Floods Peaks over time\n",
        "\n",
        "The same process is repeated, adjusted to the specific indicator."
      ],
      "id": "5df56adc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-polar-20\n",
        "#| fig-cap: Large Floods Peaks over time at station 9423001\n",
        "indicator_names = {\n",
        "    'lsq_row': 'Large flood peak',              \n",
        "    'ann_column': 'Lfld1 peak',           \n",
        "    'sco_row': 'Large flood peak',              \n",
        "    'plot_label': 'Large flood peak'            \n",
        "}\n",
        "\n",
        "def extract_data(lsq_file, ann_file, sco_file, indicator):\n",
        "    lsq_data = pd.read_csv(lsq_file, skiprows=3)\n",
        "    lsq_data.set_index(lsq_data.columns[0], inplace=True)\n",
        "    try:\n",
        "        lsq_selected = lsq_data.loc[indicator['lsq_row'], ['Slope', 'YInt', 'Sigma', 'Corr', 'PValue', 'FStat', 'R2']]\n",
        "    except KeyError:\n",
        "        print(f\"Indicator '{indicator['lsq_row']}' not found in lsq_file.\")\n",
        "        return None, None, None\n",
        "\n",
        "    ann_data = pd.read_csv(ann_file, skiprows=4)\n",
        "    ann_data.columns = ann_data.columns.str.strip()\n",
        "    if indicator['ann_column'] not in ann_data.columns:\n",
        "        print(f\"Column '{indicator['ann_column']}' not found in ann_file.\")\n",
        "        return None, None, None\n",
        "    ann_data = ann_data[['Year', indicator['ann_column']]].dropna()\n",
        "\n",
        "    sco_data = pd.read_csv(sco_file, skiprows=85, nrows=13, header=None, index_col=0)\n",
        "    sco_data.index.name = 'Parameter'\n",
        "    sco_data.columns = ['Mediana', 'Coef. Disper.']\n",
        "    if indicator['sco_row'] not in sco_data.index:\n",
        "        print(f\"Indicator '{indicator['sco_row']}' not found in sco_file.\")\n",
        "        return None, None, None\n",
        "    sco_selected = sco_data.loc[[indicator['sco_row']]]\n",
        "\n",
        "    return lsq_selected, ann_data, sco_selected\n",
        "\n",
        "def save_to_csv(lsq_data, ann_data, sco_data, output_file):\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        \n",
        "        writer.writerow(['Variable', 'Valor'])\n",
        "        for idx, value in lsq_data.items():\n",
        "            writer.writerow([idx, value])\n",
        "        \n",
        "        writer.writerow(['Year', indicator_names['ann_column']])\n",
        "        for _, row in ann_data.iterrows():\n",
        "            writer.writerow([int(row['Year']), row[indicator_names['ann_column']]])\n",
        "        \n",
        "        writer.writerow(['Mediana', sco_data['Mediana'].values[0]])\n",
        "        writer.writerow(['Coef. Disper.', sco_data['Coef. Disper.'].values[0]])\n",
        "\n",
        "def plot_data(df, slope, intercept, r_squared, station_code, plot_label):\n",
        "    df_anios = df[(df['Variable'].str.isnumeric()) & (df['Valor'].notna())].copy()\n",
        "    df_anios['Variable'] = pd.to_numeric(df_anios['Variable'], errors='coerce')\n",
        "    df_anios['Valor'] = pd.to_numeric(df_anios['Valor'], errors='coerce')\n",
        "    df_anios = df_anios.dropna()\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(df_anios['Variable'], df_anios['Valor'], marker='o', linestyle='None', color='black', label=plot_label)\n",
        "    \n",
        "    for i in range(1, len(df_anios)):\n",
        "        if df_anios['Variable'].iloc[i] - df_anios['Variable'].iloc[i-1] == 1:\n",
        "            plt.plot(df_anios['Variable'].iloc[i-1:i+1], df_anios['Valor'].iloc[i-1:i+1], linestyle='-', color='black')\n",
        "\n",
        "    line = slope * df_anios['Variable'] + intercept\n",
        "    plt.plot(df_anios['Variable'], line, linestyle='--', color='black', label='Regression')\n",
        "\n",
        "    plt.rcParams.update({'font.size': 24, 'font.family': 'Times New Roman'})\n",
        "    #plt.title(f'{plot_label} ({station_code})', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xlabel('Year', fontsize=24, fontname='Times New Roman')\n",
        "    plt.ylabel('Flow (m3/s)', fontsize=24, fontname='Times New Roman')\n",
        "    plt.xticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.yticks(fontsize=24, fontname='Times New Roman')\n",
        "    plt.legend(bbox_to_anchor=(1.35, 0.95), fontsize=14, frameon=False)\n",
        "    plt.grid(False)\n",
        "\n",
        "    info_text = f'Slope: {slope:.2f}\\nR-Squared: {r_squared:.2f}'\n",
        "    plt.text(1.15, 0.65, info_text, transform=plt.gca().transAxes, fontsize=14,\n",
        "             verticalalignment='top', horizontalalignment='center',\n",
        "             bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "    station_folder = os.path.join(output_folder_path, station_code)\n",
        "    os.makedirs(station_folder, exist_ok=True)\n",
        "\n",
        "    graph_filename = os.path.join(station_folder, f\"{plot_label.lower().replace(' ', '_')}_{station_code}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(graph_filename)\n",
        "\n",
        "    print(f\"The CSV files and figure for station {station_code} has been saved in {graph_filename}\")\n",
        "\n",
        "def process_all_stations(base_folder_path, output_folder_path, indicator):\n",
        "    station_folders = [f for f in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, f))]\n",
        "    contador = 0\n",
        "    \n",
        "    for station_code in station_folders:\n",
        "        lsq_file = os.path.join(base_folder_path, station_code, f'lsq_{station_code}.csv')\n",
        "        ann_file = os.path.join(base_folder_path, station_code, f'ann_{station_code}.csv')\n",
        "        sco_file = os.path.join(base_folder_path, station_code, f'sco_{station_code}.csv')\n",
        "\n",
        "        if not all(os.path.exists(f) for f in [lsq_file, ann_file, sco_file]):\n",
        "            print(f'Missing files for station {station_code}. Skipping station.')\n",
        "            continue\n",
        "\n",
        "        lsq_data, ann_data, sco_data = extract_data(lsq_file, ann_file, sco_file, indicator)\n",
        "        if lsq_data is None or ann_data is None or sco_data is None:\n",
        "            continue\n",
        "\n",
        "        output_file = os.path.join(output_folder_path, f\"{indicator['ann_column'].lower()}_{station_code}.csv\")\n",
        "        save_to_csv(lsq_data, ann_data, sco_data, output_file)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(output_file, names=['Variable', 'Valor'], skiprows=1)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {output_file}')\n",
        "            continue\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        try:\n",
        "            slope = float(df[df['Variable'] == 'Slope']['Valor'].values[0])\n",
        "            intercept = float(df[df['Variable'] == 'YInt']['Valor'].values[0])\n",
        "            r_squared = float(df[df['Variable'] == 'R2']['Valor'].values[0])\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f'Error extracting regression values: {e}')\n",
        "            continue\n",
        "\n",
        "        plot_data(df, slope, intercept, r_squared, station_code, indicator['plot_label'])\n",
        "        \n",
        "        if contador == 2:\n",
        "           plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "    \n",
        "        contador += 1 \n",
        "\n",
        "process_all_stations(base_folder_path, output_folder_path, indicator_names)"
      ],
      "id": "fig-polar-20",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/andrearedel/env/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}